"""
Decision Tree Algorithm Implementation

Decision trees are a non-parametric supervised learning method used for 
classification and regression. They create a model that predicts the target 
variable by learning simple decision rules inferred from the data features.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report,
                           mean_squared_error, r2_score, mean_absolute_error)
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_regression
import streamlit as st
import seaborn as sns


class DecisionTree:
    """
    Decision Tree implementation with educational explanations.
    
    Decision trees work by recursively partitioning the feature space into 
    regions and making predictions based on the majority class (classification)
    or mean value (regression) in each region.
    
    Key Concepts:
    - Root Node: Top node representing the entire dataset
    - Internal Nodes: Nodes with conditions/splits
    - Leaf Nodes: Terminal nodes with predictions
    - Splitting Criteria: Rules to divide data (Gini, Entropy, MSE)
    """
    
    def __init__(self, task_type='classification', max_depth=3, min_samples_split=2):
        self.task_type = task_type
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.model = None
        self.is_fitted = False
        
    @staticmethod
    def get_theory():
        """Return comprehensive theoretical explanation of Decision Trees."""
        return {
            'name': 'Decision Tree',
            'type': 'Supervised Learning - Classification/Regression',
            
            # 1. What the algorithm is
            'definition': """
            ğŸ”¹ **What is Decision Tree?**
            Decision Tree is like a flowchart that asks yes/no questions to make decisions.
            It starts with one question, then branches into more specific questions until 
            it reaches a final answer. Think of it as a "choose your own adventure" book for data!
            """,
            
            # 2. Why the algorithm is used
            'motivation': """
            ğŸ”¹ **Why Use Decision Trees?**
            â€¢ ğŸ¥ **Medical Diagnosis**: "Is fever > 101Â°F? â†’ Is cough present?" â†’ Diagnosis
            â€¢ ğŸ’³ **Loan Approval**: Income, credit score, age â†’ Approve/reject decision
            â€¢ ğŸ¯ **Marketing**: Customer behavior â†’ Target specific campaigns
            â€¢ ğŸŒ¦ï¸ **Weather Prediction**: Temperature, humidity, pressure â†’ Rain/shine
            â€¢ ğŸ® **Game AI**: Player moves â†’ Best counter-strategy
            â€¢ ğŸ“Š **Feature Selection**: Identifies most important variables automatically
            """,
            
            # 3. Intuition with real-life analogy
            'intuition': """
            ğŸ”¹ **Real-Life Analogy: The Animal Guessing Game**
            
            Imagine playing "20 Questions" to guess an animal:
            
            ğŸ¯ **The Game**: Guess any animal with yes/no questions
            ğŸ§  **Your Strategy**: Ask questions that split animals into groups
            
            **Decision Tree is like a master player who:**
            
            **Level 1**: ğŸ¦… "Does it fly?" 
            â”œâ”€ Yes â†’ Birds, bats, insects
            â””â”€ No â†’ Land/water animals
            
            **Level 2**: ğŸ¾ "Does it have fur?"
            â”œâ”€ Yes â†’ Mammals (cats, dogs, bears)
            â””â”€ No â†’ Reptiles, fish, etc.
            
            **Level 3**: ğŸ  "Is it a pet?"
            â”œâ”€ Yes â†’ Cat, Dog, Hamster
            â””â”€ No â†’ Wild animals
            
            **Final Guess**: "It's a DOG!" ğŸ•
            
            ğŸ¯ **In data terms**: 
            - Questions = Feature Tests
            - Animal Categories = Classes
            - Final Guess = Prediction
            - Question Strategy = Splitting Algorithm
            """,
            
            # 4. Mathematical foundation
            'math_foundation': """
            ğŸ”¹ **Mathematical Foundation (Step-by-Step)**
            
            **Core Concept: Information Gain**
            ```
            Information Gain = Entropy(Parent) - Weighted_Average(Entropy(Children))
            ```
            
            **Where:**
            â€¢ `Entropy` = Measure of disorder/uncertainty
            â€¢ `Parent` = Current node before split
            â€¢ `Children` = Nodes after split
            â€¢ `Weighted_Average` = Based on number of samples in each child
            
            **Entropy Formula (Classification):**
            ```
            Entropy(S) = -Î£(pi Ã— log2(pi))
            ```
            Where `pi` = proportion of samples in class i
            
            **Gini Impurity (Alternative):**
            ```
            Gini(S) = 1 - Î£(piÂ²)
            ```
            
            **Best Split Selection:**
            ```
            For each feature:
                For each possible threshold:
                    Calculate Information Gain
            Choose feature + threshold with highest gain
            ```
            
            **Regression (MSE):**
            ```
            MSE = (1/n) Ã— Î£(yi - È³)Â²
            MSE_Reduction = MSE(Parent) - Weighted_Average(MSE(Children))
            ```
            """,
            
            # 5. Step-by-step working
            'algorithm_steps': """
            ğŸ”¹ **How Decision Tree Works (Step-by-Step)**
            
            **Step 1: Start with Root** ğŸŒ±
            â€¢ Begin with entire dataset at root node
            â€¢ Calculate current impurity (entropy/gini/MSE)
            â€¢ Set current node as "best guess" (majority class or mean)
            
            **Step 2: Find Best Split** ğŸ”
            â€¢ For EVERY feature:
            â€¢ For EVERY possible threshold:
            â€¢ Calculate information gain or MSE reduction
            â€¢ Choose feature + threshold with maximum gain
            
            **Step 3: Create Children** ğŸ‘¶ğŸ‘¶
            â€¢ Split data into left and right child nodes
            â€¢ Left child: samples satisfying condition
            â€¢ Right child: samples NOT satisfying condition
            
            **Step 4: Recursive Splitting** ğŸ”„
            â€¢ Repeat Steps 2-3 for each child node
            â€¢ Continue until stopping criteria met:
            
            **Step 5: Stopping Criteria** âœ‹
            â€¢ Maximum depth reached
            â€¢ Minimum samples per node reached
            â€¢ No more information gain possible
            â€¢ Pure nodes achieved (all same class)
            
            **Step 6: Make Predictions** ğŸ¯
            â€¢ Start at root with new sample
            â€¢ Follow decision path down tree
            â€¢ Reach leaf node â†’ return prediction
            """,
            
            # 6. Pseudocode
            'pseudocode': """
            ğŸ”¹ **Pseudocode (Easy to Understand)**
            
            ```
            ALGORITHM: Decision Tree
            
            INPUT: 
                - X: feature matrix (n_samples Ã— n_features)
                - y: target values (n_samples Ã— 1)
                - max_depth: maximum tree depth
                - min_samples: minimum samples to split
            
            OUTPUT:
                - trained_tree: decision tree model
            
            BEGIN
                1. CREATE root node with all data
                
                2. FUNCTION build_tree(node, depth):
                   a. IF stopping_criteria_met(node, depth):
                      RETURN make_leaf(node)  # majority class or mean
                   
                   b. best_gain = 0
                   c. best_split = None
                   
                   d. FOR each feature in X:
                      FOR each threshold in unique_values(feature):
                          left_data, right_data = split_data(feature, threshold)
                          gain = calculate_information_gain(left_data, right_data)
                          IF gain > best_gain:
                              best_gain = gain
                              best_split = (feature, threshold)
                   
                   e. IF best_gain == 0:
                      RETURN make_leaf(node)
                   
                   f. CREATE left_child, right_child from best_split
                   g. left_subtree = build_tree(left_child, depth+1)
                   h. right_subtree = build_tree(right_child, depth+1)
                   
                   i. RETURN internal_node(best_split, left_subtree, right_subtree)
                
                3. tree = build_tree(root, 0)
                4. RETURN tree
            END
            
            PREDICTION:
            BEGIN
                1. START at root node
                2. WHILE current_node is not leaf:
                   IF sample[feature] <= threshold:
                       current_node = left_child
                   ELSE:
                       current_node = right_child
                3. RETURN leaf_prediction
            END
            ```
            """,
            
            # 7. Python implementation
            'python_implementation': """
            ğŸ”¹ **Python Implementation**
            
            **From Scratch (Simplified):**
            ```python
            import numpy as np
            from collections import Counter
            
            class SimpleDecisionTree:
                def __init__(self, max_depth=3, min_samples=2):
                    self.max_depth = max_depth
                    self.min_samples = min_samples
                    self.tree = None
                
                def entropy(self, y):
                    \"\"\"Calculate entropy for classification.\"\"\"
                    counts = Counter(y)
                    total = len(y)
                    entropy = 0
                    for count in counts.values():
                        p = count / total
                        if p > 0:
                            entropy -= p * np.log2(p)
                    return entropy
                
                def information_gain(self, y_parent, y_left, y_right):
                    \"\"\"Calculate information gain from a split.\"\"\"
                    n = len(y_parent)
                    n_left, n_right = len(y_left), len(y_right)
                    
                    if n_left == 0 or n_right == 0:
                        return 0
                    
                    gain = self.entropy(y_parent)
                    gain -= (n_left/n) * self.entropy(y_left)
                    gain -= (n_right/n) * self.entropy(y_right)
                    return gain
                
                def best_split(self, X, y):
                    \"\"\"Find best feature and threshold to split.\"\"\"
                    best_gain = 0
                    best_feature = None
                    best_threshold = None
                    
                    for feature in range(X.shape[1]):
                        thresholds = np.unique(X[:, feature])
                        for threshold in thresholds:
                            left_mask = X[:, feature] <= threshold
                            y_left = y[left_mask]
                            y_right = y[~left_mask]
                            
                            gain = self.information_gain(y, y_left, y_right)
                            if gain > best_gain:
                                best_gain = gain
                                best_feature = feature
                                best_threshold = threshold
                    
                    return best_feature, best_threshold, best_gain
                
                def build_tree(self, X, y, depth=0):
                    \"\"\"Recursively build the decision tree.\"\"\"
                    # Stopping criteria
                    if (depth >= self.max_depth or 
                        len(y) < self.min_samples or 
                        len(np.unique(y)) == 1):
                        return Counter(y).most_common(1)[0][0]  # majority class
                    
                    # Find best split
                    feature, threshold, gain = self.best_split(X, y)
                    if gain == 0:
                        return Counter(y).most_common(1)[0][0]
                    
                    # Create child nodes
                    left_mask = X[:, feature] <= threshold
                    left_tree = self.build_tree(X[left_mask], y[left_mask], depth+1)
                    right_tree = self.build_tree(X[~left_mask], y[~left_mask], depth+1)
                    
                    return {
                        'feature': feature,
                        'threshold': threshold,
                        'left': left_tree,
                        'right': right_tree
                    }
                
                def fit(self, X, y):
                    \"\"\"Train the decision tree.\"\"\"
                    self.tree = self.build_tree(X, y)
                
                def predict_sample(self, sample):
                    \"\"\"Predict single sample.\"\"\"
                    node = self.tree
                    while isinstance(node, dict):
                        if sample[node['feature']] <= node['threshold']:
                            node = node['left']
                        else:
                            node = node['right']
                    return node
                
                def predict(self, X):
                    \"\"\"Predict multiple samples.\"\"\"
                    return [self.predict_sample(sample) for sample in X]
            ```
            
            **Using Scikit-learn:**
            ```python
            from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score
            
            # Classification
            clf = DecisionTreeClassifier(max_depth=3, random_state=42)
            clf.fit(X_train, y_train)
            predictions = clf.predict(X_test)
            
            # Regression
            reg = DecisionTreeRegressor(max_depth=3, random_state=42)
            reg.fit(X_train, y_train)
            predictions = reg.predict(X_test)
            
            # View tree structure
            from sklearn.tree import export_text
            tree_rules = export_text(clf, feature_names=['feature_1', 'feature_2'])
            print(tree_rules)
            ```
            """,
            
            # 8. Example with sample input/output
            'example': """
            ğŸ”¹ **Complete Example: Tennis Playing Decision**
            
            **Input Data (Should I play tennis?):**
            ```
            Weather    | Temperature | Humidity | Wind   | Play?
            Sunny      | Hot         | High     | Weak   | No
            Sunny      | Hot         | High     | Strong | No  
            Overcast   | Hot         | High     | Weak   | Yes
            Rain       | Mild        | High     | Weak   | Yes
            Rain       | Cool        | Normal   | Weak   | Yes
            Rain       | Cool        | Normal   | Strong | No
            Overcast   | Cool        | Normal   | Strong | Yes
            Sunny      | Mild        | High     | Weak   | No
            Sunny      | Cool        | Normal   | Weak   | Yes
            Rain       | Mild        | Normal   | Weak   | Yes
            ```
            
            **Step-by-Step Tree Building:**
            ```
            1. Root Node: 14 samples (9 Yes, 5 No) â†’ Entropy = 0.94
            
            2. Best Split Found: Weather = "Sunny"?
               - Left (Sunny): 5 samples (2 Yes, 3 No) â†’ Entropy = 0.97
               - Right (Not Sunny): 9 samples (7 Yes, 2 No) â†’ Entropy = 0.76
               - Information Gain = 0.94 - (5/14)Ã—0.97 - (9/14)Ã—0.76 = 0.25
            
            3. Split Sunny Branch: Humidity = "High"?
               - Left (High): 3 samples (0 Yes, 3 No) â†’ Pure! â†’ Predict: NO
               - Right (Normal): 2 samples (2 Yes, 0 No) â†’ Pure! â†’ Predict: YES
            
            4. Split Non-Sunny Branch: Weather = "Rain"?
               - Rain + Wind Strong: NO
               - Rain + Wind Weak: YES  
               - Overcast: YES
            ```
            
            **Final Decision Tree:**
            ```
            Weather = Sunny?
            â”œâ”€ Yes: Humidity = High?
            â”‚   â”œâ”€ Yes: DON'T PLAY âŒ
            â”‚   â””â”€ No: PLAY âœ…
            â””â”€ No: Weather = Rain?
                â”œâ”€ Yes: Wind = Strong?
                â”‚   â”œâ”€ Yes: DON'T PLAY âŒ
                â”‚   â””â”€ No: PLAY âœ…
                â””â”€ No (Overcast): PLAY âœ…
            ```
            
            **New Prediction:**
            ```
            New Day: Sunny, Mild, Normal, Weak
            Path: Sunny? â†’ Yes â†’ Humidity High? â†’ No â†’ PLAY! âœ…
            ```
            """,
            
            # 9. Visualization explanation
            'visualization': """
            ğŸ”¹ **Understanding Through Visualizations**
            
            **Tree Structure Plot:**
            ğŸ“Š Shows complete decision flow
            â€¢ Root at top, leaves at bottom
            â€¢ Each box shows: splitting condition, samples, predicted class
            â€¢ Colors indicate class purity (darker = more pure)
            â€¢ Path from root to leaf = decision sequence
            
            **Feature Importance Chart:**
            ğŸ“ˆ Bar chart showing feature contributions
            â€¢ Height = how much feature reduces impurity
            â€¢ Features used higher in tree = more important
            â€¢ Sum of all importances = 1.0
            
            **Decision Boundaries (2D features):**
            ğŸ¯ Rectangular regions in feature space
            â€¢ Each region = one leaf node
            â€¢ Parallel lines to axes (axis-aligned splits)
            â€¢ Different colors = different predicted classes
            
            **Confusion Matrix (Classification):**
            ğŸ“‹ Actual vs Predicted class counts
            â€¢ Diagonal = correct predictions
            â€¢ Off-diagonal = errors
            â€¢ Perfect tree = only diagonal values
            
            **Residual Plots (Regression):**
            ğŸ“‰ Prediction errors vs predicted values
            â€¢ Good tree = random scatter around zero
            â€¢ Patterns indicate underfitting or overfitting
            """,
            
            # 10. Time and space complexity
            'complexity': """
            ğŸ”¹ **Time & Space Complexity**
            
            **Time Complexity:**
            â€¢ **Training**: O(n Ã— m Ã— log(n)) where n=samples, m=features
            â€¢ **Prediction**: O(log(n)) average, O(n) worst case (unbalanced tree)
            â€¢ **Tree Building**: O(n Ã— m Ã— d) where d=depth
            â€¢ **Split Finding**: O(n Ã— log(n)) per feature per node
            
            **Space Complexity:**
            â€¢ **Model Storage**: O(nodes) = O(2^d) worst case, O(log(n)) average
            â€¢ **Training Memory**: O(n Ã— m) to store dataset
            â€¢ **Recursion Stack**: O(d) for tree building
            
            **Scalability:**
            â€¢ âœ… **Fast Prediction**: Logarithmic time for most trees
            â€¢ âš ï¸ **Training Time**: Can be slow with many features
            â€¢ âœ… **Memory Efficient**: Only stores split conditions
            â€¢ âŒ **Unbalanced Trees**: Can degrade to linear time
            """,
            
            # 11. Advantages and disadvantages
            'pros_cons': """
            ğŸ”¹ **Advantages** âœ…
            â€¢ **Highly Interpretable**: Easy to understand decision rules
            â€¢ **No Assumptions**: Works with any data distribution
            â€¢ **Handles Mixed Data**: Numerical and categorical features
            â€¢ **Feature Selection**: Automatically identifies important features
            â€¢ **No Scaling Needed**: Robust to different feature scales
            â€¢ **Missing Values**: Can handle missing data naturally
            â€¢ **Non-linear**: Captures complex interactions and patterns
            â€¢ **Fast Predictions**: Logarithmic time complexity
            â€¢ **Rule Extraction**: Provides explicit if-then rules
            
            ğŸ”¹ **Disadvantages** âŒ
            â€¢ **Overfitting**: Creates overly complex trees on training data
            â€¢ **Instability**: Small data changes create completely different trees
            â€¢ **Bias**: Favors features with more distinct values
            â€¢ **Linear Relationships**: Poor at modeling simple linear patterns
            â€¢ **Probability Estimates**: Provides poor class probabilities
            â€¢ **Memory Growth**: Tree size can grow exponentially
            â€¢ **Extrapolation**: Cannot predict beyond training data range
            """,
            
            # 12. When to use and when NOT to use
            'usage_guide': """
            ğŸ”¹ **When TO Use Decision Trees** âœ…
            
            **Perfect for:**
            â€¢ ğŸ¯ **Rule-based Problems**: Need clear, interpretable rules
            â€¢ ğŸ¥ **Medical Diagnosis**: Doctors need explainable decisions
            â€¢ ğŸ’¼ **Business Rules**: Convert decisions into business logic
            â€¢ ğŸ” **Feature Exploration**: Understanding which features matter
            â€¢ ğŸ“Š **Mixed Data Types**: Combination of numerical and categorical
            â€¢ ğŸš€ **Prototyping**: Quick baseline for any classification/regression
            
            **Good when:**
            â€¢ Interpretability is more important than accuracy
            â€¢ Data has complex non-linear interactions
            â€¢ Features have different scales and types
            â€¢ You need automatic feature selection
            â€¢ Training data is limited
            
            ğŸ”¹ **When NOT to Use Decision Trees** âŒ
            
            **Avoid when:**
            â€¢ ğŸ“ **Linear Relationships**: Simple linear patterns (use Linear Regression)
            â€¢ ğŸ¯ **High Accuracy Needed**: Better algorithms available (Random Forest)
            â€¢ ğŸ“Š **Small Datasets**: Prone to overfitting with limited data
            â€¢ ğŸ² **High Noise**: Unstable with very noisy data
            â€¢ ğŸ“ˆ **Continuous Smooth Functions**: Cannot model smooth curves
            â€¢ âš¡ **Real-time Learning**: Need to update model frequently
            
            **Use instead:**
            â€¢ Linear models (for linear relationships)
            â€¢ Random Forest (for better accuracy)
            â€¢ Neural Networks (for complex patterns)
            â€¢ Ensemble methods (for stability)
            """,
            
            # 13. Common interview questions
            'interview_questions': """
            ğŸ”¹ **Common Interview Questions & Answers**
            
            **Q1: How do you prevent overfitting in decision trees?**
            A: 
            â€¢ Pre-pruning: Set max_depth, min_samples_split, min_samples_leaf
            â€¢ Post-pruning: Build full tree then remove unnecessary branches
            â€¢ Cross-validation: Use validation data to select optimal parameters
            â€¢ Ensemble methods: Use Random Forest instead of single tree
            
            **Q2: What's the difference between Gini and Entropy?**
            A:
            â€¢ Gini: Faster to compute, range [0, 0.5], prefers largest class
            â€¢ Entropy: More theoretically grounded, range [0, 1], better balanced splits
            â€¢ In practice: Very similar results, Gini slightly faster
            
            **Q3: Why are decision trees unstable?**
            A: Small changes in training data can create completely different trees because:
            â€¢ Greedy algorithm: Makes locally optimal decisions
            â€¢ Hierarchical structure: Early split changes affect entire subtree
            â€¢ Solution: Use ensemble methods like Random Forest
            
            **Q4: How do decision trees handle continuous features?**
            A: 
            â€¢ Sort unique values of feature
            â€¢ Try all possible thresholds as split points
            â€¢ Choose threshold that maximizes information gain
            â€¢ Creates binary splits: â‰¤ threshold vs > threshold
            
            **Q5: Can decision trees do feature selection?**
            A: Yes! Features not used in any split have zero importance. Features used higher in tree or in more nodes have higher importance. Tree automatically ignores irrelevant features.
            """,
            
            # 14. Common mistakes
            'common_mistakes': """
            ğŸ”¹ **Common Beginner Mistakes & How to Avoid**
            
            **Mistake 1: Not limiting tree depth** ğŸš«
            âŒ Letting tree grow too deep and memorizing training data
            âœ… **Fix**: Set max_depth=5-10, use validation to find optimal depth
            
            **Mistake 2: Ignoring class imbalance** ğŸš«
            âŒ Tree biased toward majority class with imbalanced data
            âœ… **Fix**: Use class_weight='balanced' or stratified sampling
            
            **Mistake 3: Using single tree for production** ğŸš«
            âŒ Single trees are unstable and overfit easily
            âœ… **Fix**: Use Random Forest or Gradient Boosting instead
            
            **Mistake 4: Not validating splits** ğŸš«
            âŒ Trusting tree performance on training data only
            âœ… **Fix**: Always use cross-validation or separate test set
            
            **Mistake 5: Over-interpreting feature importance** ğŸš«
            âŒ "This feature is unimportant because tree didn't use it"
            âœ… **Fix**: Consider feature interactions, try multiple tree configurations
            
            **Mistake 6: Expecting smooth predictions** ğŸš«
            âŒ Trees create step-wise predictions, not smooth curves
            âœ… **Fix**: Use ensemble methods or other algorithms for smooth functions
            """,
            
            # 15. Comparison with similar algorithms
            'comparisons': """
            ğŸ”¹ **Decision Trees vs Similar Algorithms**
            
            **Decision Tree vs Random Forest:**
            â€¢ **Decision Tree**: Single tree, interpretable, unstable
            â€¢ **Random Forest**: Multiple trees, more accurate, less interpretable
            â€¢ **Use Random Forest**: When accuracy > interpretability
            
            **Decision Tree vs Logistic Regression:**
            â€¢ **Decision Tree**: Captures non-linear patterns, no assumptions
            â€¢ **Logistic Regression**: Linear decision boundary, probabilistic
            â€¢ **Use Logistic**: When relationship is roughly linear
            
            **Decision Tree vs K-Nearest Neighbors:**
            â€¢ **Decision Tree**: Creates explicit rules, fast prediction
            â€¢ **KNN**: Instance-based, no training phase, smooth boundaries
            â€¢ **Use KNN**: When local patterns matter more than global rules
            
            **Decision Tree vs Neural Networks:**
            â€¢ **Decision Tree**: Interpretable, handles mixed data easily
            â€¢ **Neural Networks**: More flexible, handles complex patterns
            â€¢ **Use Neural Networks**: When you have lots of data and need high accuracy
            
            **Decision Tree vs Naive Bayes:**
            â€¢ **Decision Tree**: No independence assumptions, handles interactions
            â€¢ **Naive Bayes**: Assumes feature independence, probabilistic
            â€¢ **Use Naive Bayes**: When features are mostly independent
            """,
            
            # 16. Real-world applications
            'real_world_applications': """
            ğŸ”¹ **Real-World Applications & Industry Use Cases**
            
            **ğŸ¥ Healthcare & Medicine:**
            â€¢ Medical diagnosis decision support systems
            â€¢ Drug dosage determination based on patient factors
            â€¢ Treatment path recommendations
            â€¢ Clinical trial patient stratification
            â€¢ Epidemic outbreak prediction
            
            **ğŸ’° Finance & Banking:**
            â€¢ Credit approval and loan default prediction
            â€¢ Fraud detection in transactions
            â€¢ Investment portfolio risk assessment
            â€¢ Insurance claim processing
            â€¢ Algorithmic trading rule generation
            
            **ğŸ›’ E-commerce & Retail:**
            â€¢ Customer segmentation and targeting
            â€¢ Product recommendation systems
            â€¢ Inventory management decisions
            â€¢ Price optimization strategies
            â€¢ Supply chain optimization
            
            **ğŸ­ Manufacturing & Operations:**
            â€¢ Quality control and defect detection
            â€¢ Predictive maintenance scheduling
            â€¢ Production planning optimization
            â€¢ Equipment failure diagnosis
            â€¢ Process control automation
            
            **ğŸ“± Technology & Software:**
            â€¢ User behavior analysis
            â€¢ Feature flagging and A/B testing
            â€¢ Content recommendation engines
            â€¢ Cybersecurity threat detection
            â€¢ Resource allocation in cloud services
            
            **ğŸ“ Education & Research:**
            â€¢ Student performance prediction
            â€¢ Curriculum design optimization
            â€¢ Learning path personalization
            â€¢ Research data mining
            â€¢ Academic intervention systems
            
            **ğŸŒ± Agriculture & Environment:**
            â€¢ Crop yield prediction
            â€¢ Pest and disease identification
            â€¢ Weather pattern analysis
            â€¢ Environmental monitoring
            â€¢ Species classification
            
            **ğŸ’¡ Key Success Factors:**
            â€¢ Domain expertise for feature engineering
            â€¢ Proper tree pruning to prevent overfitting
            â€¢ Regular model validation and updating
            â€¢ Ensemble methods for production systems
            â€¢ Clear documentation of decision rules
            """
        }
    
    def generate_sample_data(self, task_type, n_samples=300, n_features=4):
        """Generate sample data for demonstration."""
        if task_type == 'classification':
            X, y = make_classification(
                n_samples=n_samples,
                n_features=n_features,
                n_informative=max(2, n_features // 2),
                n_redundant=0,
                n_clusters_per_class=1,
                random_state=42
            )
        else:  # regression
            X, y = make_regression(
                n_samples=n_samples,
                n_features=n_features,
                noise=0.1,
                random_state=42
            )
        return X, y
    
    def fit(self, X, y):
        """Fit the decision tree model."""
        if self.task_type == 'classification':
            self.model = DecisionTreeClassifier(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                random_state=42
            )
        else:
            self.model = DecisionTreeRegressor(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                random_state=42
            )
        
        self.model.fit(X, y)
        self.is_fitted = True
        return self
    
    def predict(self, X):
        """Make predictions using the fitted model."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")
        return self.model.predict(X)
    
    def predict_proba(self, X):
        """Predict class probabilities (classification only)."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")
        if self.task_type != 'classification':
            raise ValueError("Probability prediction only available for classification")
        return self.model.predict_proba(X)
    
    def get_metrics(self, X, y):
        """Calculate and return model performance metrics."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before calculating metrics")
            
        y_pred = self.predict(X)
        
        if self.task_type == 'classification':
            metrics = {
                'Accuracy': accuracy_score(y, y_pred),
                'Precision': precision_score(y, y_pred, average='weighted', zero_division=0),
                'Recall': recall_score(y, y_pred, average='weighted', zero_division=0),
                'F1-Score': f1_score(y, y_pred, average='weighted', zero_division=0)
            }
        else:  # regression
            metrics = {
                'Mean Squared Error (MSE)': mean_squared_error(y, y_pred),
                'Root Mean Squared Error (RMSE)': np.sqrt(mean_squared_error(y, y_pred)),
                'Mean Absolute Error (MAE)': mean_absolute_error(y, y_pred),
                'RÂ² Score': r2_score(y, y_pred)
            }
        
        return metrics
    
    def plot_tree_structure(self, feature_names=None):
        """Plot the decision tree structure."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before plotting tree")
            
        if feature_names is None:
            n_features = self.model.n_features_in_
            feature_names = [f'Feature {i+1}' for i in range(n_features)]
        
        fig, ax = plt.subplots(figsize=(20, 12))
        
        plot_tree(
            self.model,
            feature_names=feature_names,
            filled=True,
            rounded=True,
            fontsize=10,
            ax=ax
        )
        
        ax.set_title(f'Decision Tree Structure (max_depth={self.max_depth})', 
                    fontsize=16, fontweight='bold')
        
        return fig
    
    def get_tree_rules(self, feature_names=None):
        """Extract decision rules as text."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before extracting rules")
            
        if feature_names is None:
            n_features = self.model.n_features_in_
            feature_names = [f'Feature {i+1}' for i in range(n_features)]
        
        return export_text(self.model, feature_names=feature_names)
    
    def plot_feature_importance(self, feature_names=None):
        """Plot feature importance from the decision tree."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before plotting feature importance")
            
        importances = self.model.feature_importances_
        
        if feature_names is None:
            feature_names = [f'Feature {i+1}' for i in range(len(importances))]
        
        # Sort features by importance
        indices = np.argsort(importances)[::-1]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        bars = ax.bar(range(len(importances)), importances[indices], 
                     alpha=0.7, color='lightgreen', edgecolor='black')
        
        ax.set_xlabel('Features')
        ax.set_ylabel('Importance')
        ax.set_title('Feature Importance in Decision Tree')
        ax.set_xticks(range(len(importances)))
        ax.set_xticklabels([feature_names[i] for i in indices], rotation=45)
        ax.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, importance in zip(bars, importances[indices]):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{importance:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        return fig
    
    def plot_classification_results(self, X, y):
        """Create classification visualization (classification only)."""
        if self.task_type != 'classification':
            raise ValueError("Classification plots only available for classification tasks")
            
        if not self.is_fitted:
            raise ValueError("Model must be fitted before plotting")
            
        y_pred = self.predict(X)
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Confusion Matrix
        cm = confusion_matrix(y, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
        axes[0].set_xlabel('Predicted')
        axes[0].set_ylabel('Actual')
        axes[0].set_title('Confusion Matrix')
        
        # Class distribution
        unique_classes = np.unique(y)
        class_counts_actual = [np.sum(y == cls) for cls in unique_classes]
        class_counts_pred = [np.sum(y_pred == cls) for cls in unique_classes]
        
        x = np.arange(len(unique_classes))
        width = 0.35
        
        axes[1].bar(x - width/2, class_counts_actual, width, 
                   label='Actual', alpha=0.7, color='skyblue', edgecolor='black')
        axes[1].bar(x + width/2, class_counts_pred, width,
                   label='Predicted', alpha=0.7, color='lightcoral', edgecolor='black')
        
        axes[1].set_xlabel('Class')
        axes[1].set_ylabel('Count')
        axes[1].set_title('Class Distribution: Actual vs Predicted')
        axes[1].set_xticks(x)
        axes[1].set_xticklabels([f'Class {cls}' for cls in unique_classes])
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def plot_regression_results(self, X, y):
        """Create regression visualization (regression only)."""
        if self.task_type != 'regression':
            raise ValueError("Regression plots only available for regression tasks")
            
        if not self.is_fitted:
            raise ValueError("Model must be fitted before plotting")
            
        y_pred = self.predict(X)
        residuals = y - y_pred
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Actual vs Predicted
        axes[0, 0].scatter(y, y_pred, alpha=0.6, color='blue')
        axes[0, 0].plot([y.min(), y.max()], [y.min(), y.max()], 
                       'r--', linewidth=2, label='Perfect Prediction')
        axes[0, 0].set_xlabel('Actual')
        axes[0, 0].set_ylabel('Predicted')
        axes[0, 0].set_title('Actual vs Predicted')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Residual plot
        axes[0, 1].scatter(y_pred, residuals, alpha=0.6, color='green')
        axes[0, 1].axhline(y=0, color='red', linestyle='--')
        axes[0, 1].set_xlabel('Predicted Values')
        axes[0, 1].set_ylabel('Residuals')
        axes[0, 1].set_title('Residual Plot')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Histogram of residuals
        axes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='orange')
        axes[1, 0].axvline(np.mean(residuals), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(residuals):.4f}')
        axes[1, 0].set_xlabel('Residuals')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Distribution of Residuals')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Feature importance (if tree is fitted)
        if hasattr(self.model, 'feature_importances_'):
            feature_names = [f'Feature {i+1}' for i in range(X.shape[1])]
            importances = self.model.feature_importances_
            
            bars = axes[1, 1].bar(feature_names, importances, alpha=0.7, 
                                color='lightgreen', edgecolor='black')
            axes[1, 1].set_xlabel('Features')
            axes[1, 1].set_ylabel('Importance')
            axes[1, 1].set_title('Feature Importance')
            axes[1, 1].grid(True, alpha=0.3)
            
            # Rotate x-axis labels if many features
            if len(feature_names) > 5:
                plt.setp(axes[1, 1].get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        return fig
    
    def streamlit_interface(self):
        """Create comprehensive Streamlit interface for Decision Trees."""
        st.subheader("ğŸŒ³ Decision Tree")
        
        theory = self.get_theory()
        
        # Main tabs for comprehensive coverage
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "ğŸ¯ Overview", "ğŸ“š Deep Dive", "ğŸ’» Implementation", 
            "ğŸ§ª Interactive Demo", "â“ Q&A", "ğŸ¢ Applications"
        ])
        
        with tab1:
            # Overview Tab - Essential Information
            st.markdown("### ğŸ¯ What is Decision Tree?")
            st.markdown(theory['definition'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### ğŸŒŸ Why Use It?")
                st.markdown(theory['motivation'])
                
            with col2:
                st.markdown("### ğŸ® Simple Analogy")
                st.markdown(theory['intuition'])
            
            # Quick advantages/disadvantages
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### âœ… Pros")
                st.markdown(theory['pros_cons'].split('ğŸ”¹ **Disadvantages**')[0])
                
            with col2:
                st.markdown("### âŒ Cons")
                if 'ğŸ”¹ **Disadvantages**' in theory['pros_cons']:
                    st.markdown("ğŸ”¹ **Disadvantages**" + theory['pros_cons'].split('ğŸ”¹ **Disadvantages**')[1])
        
        with tab2:
            # Deep Dive Tab - Mathematical and Technical Details
            st.markdown("### ğŸ“Š Mathematical Foundation")
            st.markdown(theory['math_foundation'])
            
            st.markdown("### ğŸ”„ Algorithm Steps")
            st.markdown(theory['algorithm_steps'])
            
            st.markdown("### ğŸ’¾ Pseudocode")
            st.markdown(theory['pseudocode'])
            
            st.markdown("### âš¡ Time & Space Complexity")
            st.markdown(theory['complexity'])
            
        with tab3:
            # Implementation Tab
            st.markdown("### ğŸ’» Python Implementation")
            st.markdown(theory['python_implementation'])
            
            st.markdown("### ğŸ“‹ Complete Example")
            st.markdown(theory['example'])
            
            st.markdown("### ğŸ“ˆ Visualization Guide")
            st.markdown(theory['visualization'])
        
        with tab4:
            # Interactive Demo Tab
            st.markdown("### ğŸ§ª Try Decision Tree Yourself!")
            self._create_interactive_demo()
        
        with tab5:
            # Q&A Tab
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### ğŸ¯ When to Use")
                st.markdown(theory['usage_guide'])
                
                st.markdown("### ğŸš« Common Mistakes")
                st.markdown(theory['common_mistakes'])
                
            with col2:
                st.markdown("### â“ Interview Questions")
                st.markdown(theory['interview_questions'])
                
                st.markdown("### âš–ï¸ Algorithm Comparisons")
                st.markdown(theory['comparisons'])
        
        with tab6:
            # Applications Tab
            st.markdown("### ğŸŒ Real-World Applications")
            st.markdown(theory['real_world_applications'])
    
    def _create_interactive_demo(self):
        """Create the interactive demo section."""
        # Parameters section
        st.markdown("### ğŸ”§ Parameters")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            task_type = st.selectbox("Task Type:", ['classification', 'regression'])
            
        with col2:
            n_samples = st.slider("Number of samples:", 100, 1000, 300)
            
        with col3:
            n_features = st.slider("Number of features:", 2, 8, 4)
            
        with col4:
            max_depth = st.slider("Max depth:", 1, 10, 3)
        
        # Update task type
        self.task_type = task_type
        self.max_depth = max_depth
        
        # Generate and split data
        X, y = self.generate_sample_data(task_type, n_samples, n_features)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train model
        self.fit(X_train, y_train)
        
        # Results section
        st.markdown("### ğŸ“Š Results")
        
        # Metrics
        train_metrics = self.get_metrics(X_train, y_train)
        test_metrics = self.get_metrics(X_test, y_test)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Training Metrics:**")
            for metric_name, value in train_metrics.items():
                st.metric(metric_name, f"{value:.4f}")
                
        with col2:
            st.markdown("**Test Metrics:**")
            for metric_name, value in test_metrics.items():
                st.metric(metric_name, f"{value:.4f}")
        
        # Tree structure visualization
        st.markdown("### ğŸŒ² Tree Structure")
        
        # Limit tree size for better visualization
        if max_depth <= 4 and n_features <= 6:
            fig_tree = self.plot_tree_structure()
            st.pyplot(fig_tree)
            plt.close()
        else:
            st.warning("Tree too large for visualization. Showing text rules instead.")
        
        # Decision rules
        with st.expander("ğŸ“‹ Decision Rules", expanded=False):
            rules = self.get_tree_rules()
            st.text(rules)
        
        # Feature importance
        st.markdown("### ğŸ“Š Feature Importance")
        fig_importance = self.plot_feature_importance()
        st.pyplot(fig_importance)
        plt.close()
        
        # Task-specific visualizations
        st.markdown("### ğŸ“ˆ Model Performance")
        
        if task_type == 'classification':
            fig_results = self.plot_classification_results(X_test, y_test)
            st.pyplot(fig_results)
            plt.close()
            
            # Classification report
            y_pred_test = self.predict(X_test)
            report = classification_report(y_test, y_pred_test, output_dict=True)
            
            st.markdown("**Detailed Classification Report:**")
            report_df = pd.DataFrame(report).transpose()
            st.dataframe(report_df.round(4))
            
        else:  # regression
            fig_results = self.plot_regression_results(X_test, y_test)
            st.pyplot(fig_results)
            plt.close()
        
        # Interpretation
        st.markdown("### ğŸ” Interpretation")
        
        if task_type == 'classification':
            accuracy = test_metrics['Accuracy']
            if accuracy > 0.9:
                st.success(f"**Excellent performance!** Accuracy: {accuracy:.1%}")
            elif accuracy > 0.8:
                st.info(f"**Good performance.** Accuracy: {accuracy:.1%}")
            elif accuracy > 0.7:
                st.warning(f"**Moderate performance.** Accuracy: {accuracy:.1%}")
            else:
                st.error(f"**Poor performance.** Accuracy: {accuracy:.1%}")
        else:
            r2_score_val = test_metrics['RÂ² Score']
            if r2_score_val > 0.8:
                st.success(f"**Excellent fit!** RÂ² Score: {r2_score_val:.3f}")
            elif r2_score_val > 0.6:
                st.info(f"**Good fit.** RÂ² Score: {r2_score_val:.3f}")
            elif r2_score_val > 0.3:
                st.warning(f"**Moderate fit.** RÂ² Score: {r2_score_val:.3f}")
            else:
                st.error(f"**Poor fit.** RÂ² Score: {r2_score_val:.3f}")
        
        # Tree statistics
        st.markdown("**Tree Statistics:**")
        col1, col2, col3 = st.columns(3)
        col1.metric("Tree Depth", self.model.get_depth())
        col2.metric("Number of Leaves", self.model.get_n_leaves())
        col3.metric("Number of Features Used", np.sum(self.model.feature_importances_ > 0))


def main():
    """Main function for testing Decision Tree."""
    dt = DecisionTree()
    dt.streamlit_interface()


if __name__ == "__main__":
    main()