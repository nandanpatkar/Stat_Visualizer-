"""
Support Vector Machine (SVM) Algorithm Implementation

Support Vector Machines are powerful supervised learning models used for
classification and regression that work by finding the optimal hyperplane
to separate different classes.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.svm import SVC, SVR
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report,
                           mean_squared_error, r2_score, mean_absolute_error)
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_regression
from sklearn.preprocessing import StandardScaler
import streamlit as st


class SupportVectorMachine:
    """
    Support Vector Machine implementation with educational explanations.
    
    SVM finds the optimal hyperplane that maximizes the margin between
    different classes. It can handle both linear and non-linear relationships
    using kernel functions.
    """
    
    def __init__(self, task_type='classification', kernel='rbf', C=1.0):
        self.task_type = task_type
        self.kernel = kernel
        self.C = C
        self.model = None
        self.is_fitted = False
        
    @staticmethod
    def get_theory():
        """Return comprehensive theoretical explanation of Support Vector Machine."""
        return {
            'name': 'Support Vector Machine (SVM)',
            'type': 'Supervised Learning - Classification/Regression',
            
            # 1. What the algorithm is
            'definition': """
            ðŸ”¹ **What is Support Vector Machine?**
            SVM is like finding the perfect boundary line that separates different groups with the 
            maximum safety margin. Think of it as drawing the widest possible "no-man's land" 
            between opposing teams on a battlefield.
            """,
            
            # 2. Why the algorithm is used
            'motivation': """
            ðŸ”¹ **Why Use Support Vector Machine?**
            â€¢ ðŸ“„ **Text Classification**: Spam detection, document categorization
            â€¢ ðŸ–¼ï¸ **Image Recognition**: Face detection, object classification
            â€¢ ðŸ§¬ **Bioinformatics**: Gene classification, protein analysis
            â€¢ ðŸ“Š **High-Dimensional Data**: Works well when features > samples
            â€¢ ðŸŽ¯ **Small Datasets**: Effective with limited training data
            â€¢ ðŸ”¬ **Scientific Research**: Reliable for research applications
            """,
            
            # 3. Intuition with real-life analogy
            'intuition': """
            ðŸ”¹ **Real-Life Analogy: The Optimal Highway Construction**
            
            Imagine you're a civil engineer designing a highway between two cities:
            
            ðŸ™ï¸ **The Problem**: Build highway separating City A from City B
            ðŸŽ¯ **Goal**: Maximum safety margin (widest median strip possible)
            
            **SVM is like a master engineer who:**
            
            **Step 1**: ðŸ“ Measures distance to nearest buildings on each side
            **Step 2**: ðŸŽ¯ Finds the line that maximizes distance to BOTH sides
            **Step 3**: ðŸ—ï¸ Builds highway along this optimal line
            **Step 4**: ðŸ›¡ï¸ The nearest buildings become "support vectors"
            
            **Why this works:**
            â€¢ **Maximum Margin**: Safest possible separation
            â€¢ **Support Vectors**: Only critical buildings matter for placement
            â€¢ **Robust Design**: Small building changes don't affect highway
            â€¢ **Optimal Solution**: Mathematically proven best placement
            
            **For non-linear terrain** (curved boundaries):
            â€¢ Use **Kernel Trick**: Transform terrain into higher dimension
            â€¢ Find straight highway in new dimension
            â€¢ Project back to get curved path in original terrain
            
            ðŸŽ¯ **In data terms**: 
            - Cities = Classes
            - Buildings = Data Points
            - Highway = Decision Boundary
            - Nearest Buildings = Support Vectors
            - Median Strip Width = Margin
            """,
            
            # 4. Mathematical foundation
            'math_foundation': """
            ðŸ”¹ **Mathematical Foundation (Step-by-Step)**
            
            **Optimization Problem (Linear SVM):**
            ```
            Minimize: (1/2) ||w||Â² + C Î£áµ¢ Î¾áµ¢
            Subject to: yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1 - Î¾áµ¢, Î¾áµ¢ â‰¥ 0
            ```
            
            **Where:**
            â€¢ `w` = Weight vector (perpendicular to decision boundary)
            â€¢ `||w||` = Magnitude of weight vector
            â€¢ `C` = Regularization parameter (trade-off between margin and errors)
            â€¢ `Î¾áµ¢` = Slack variables (allow some misclassification)
            â€¢ `b` = Bias term (intercept)
            â€¢ `yáµ¢` = Class label (+1 or -1)
            
            **Decision Function:**
            ```
            f(x) = sign(wÂ·x + b) = sign(Î£áµ¢ Î±áµ¢ yáµ¢ K(xáµ¢, x) + b)
            ```
            
            **Margin Calculation:**
            ```
            Margin = 2 / ||w||
            # Goal: Maximize margin â†’ Minimize ||w||
            ```
            
            **Kernel Functions:**
            ```
            Linear: K(xáµ¢, xâ±¼) = xáµ¢Â·xâ±¼
            Polynomial: K(xáµ¢, xâ±¼) = (Î³ xáµ¢Â·xâ±¼ + r)áµˆ
            RBF: K(xáµ¢, xâ±¼) = exp(-Î³ ||xáµ¢ - xâ±¼||Â²)
            Sigmoid: K(xáµ¢, xâ±¼) = tanh(Î³ xáµ¢Â·xâ±¼ + r)
            ```
            
            **Support Vectors:**
            ```
            Support vectors are points where Î±áµ¢ > 0
            These are the only points that matter for the decision boundary
            ```
            
            **Dual Formulation (Lagrangian):**
            ```
            Maximize: Î£áµ¢ Î±áµ¢ - (1/2) Î£áµ¢ Î£â±¼ Î±áµ¢ Î±â±¼ yáµ¢ yâ±¼ K(xáµ¢, xâ±¼)
            Subject to: Î£áµ¢ Î±áµ¢ yáµ¢ = 0, 0 â‰¤ Î±áµ¢ â‰¤ C
            ```
            """,
            
            # 5. Step-by-step working
            'algorithm_steps': """
            ðŸ”¹ **How SVM Works (Step-by-Step)**
            
            **Step 1: Data Preparation** ðŸ“‹
            â€¢ Collect labeled training data (X, y)
            â€¢ Scale features to similar ranges (very important for SVM!)
            â€¢ Choose kernel function based on data characteristics
            
            **Step 2: Formulate Optimization** ðŸŽ¯
            â€¢ Set up constrained optimization problem
            â€¢ Goal: Find hyperplane with maximum margin
            â€¢ Balance between margin size and classification errors
            
            **Step 3: Solve Dual Problem** ðŸ”¢
            â€¢ Convert to dual optimization (easier to solve)
            â€¢ Use quadratic programming or SMO (Sequential Minimal Optimization)
            â€¢ Find Lagrange multipliers (Î±áµ¢) for each training point
            
            **Step 4: Identify Support Vectors** ðŸŽ¯
            â€¢ Support vectors are points where Î±áµ¢ > 0
            â€¢ These points lie on or inside the margin
            â€¢ Only these points influence the decision boundary
            
            **Step 5: Construct Decision Function** ðŸ“
            â€¢ Use support vectors to build decision function
            â€¢ Non-support vectors are discarded (Î±áµ¢ = 0)
            â€¢ Calculate bias term using support vectors
            
            **Step 6: Handle Non-Linear Data** ðŸŒ€
            â€¢ Apply kernel trick for non-linear separation
            â€¢ Transform data to higher dimensional space implicitly
            â€¢ Find linear separator in new space
            
            **Step 7: Make Predictions** ðŸ”®
            â€¢ For new point: f(x) = Î£áµ¢ Î±áµ¢ yáµ¢ K(xáµ¢, x) + b
            â€¢ Classification: sign(f(x)) â†’ class label
            â€¢ Regression: f(x) â†’ continuous value
            
            **Step 8: Evaluate Performance** âœ…
            â€¢ Test on unseen data
            â€¢ Monitor for overfitting (especially with complex kernels)
            """,
            
            # 6. Pseudocode
            'pseudocode': """
            ðŸ”¹ **Pseudocode (Easy to Understand)**
            
            ```
            ALGORITHM: Support Vector Machine
            
            INPUT: 
                - X: feature matrix (n_samples Ã— n_features)
                - y: target values (n_samples Ã— 1)
                - kernel: kernel function type
                - C: regularization parameter
            
            OUTPUT:
                - support_vectors: critical data points
                - alpha: Lagrange multipliers
                - bias: intercept term
            
            BEGIN
                1. PREPROCESS data:
                   X_scaled = standardize(X)  # Very important!
                
                2. SETUP optimization problem:
                   MINIMIZE: (1/2) ||w||Â² + C Ã— Î£(slack_variables)
                   SUBJECT TO: yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1 - Î¾áµ¢
                
                3. SOLVE dual problem:
                   MAXIMIZE: Î£ Î±áµ¢ - (1/2) Î£áµ¢ Î£â±¼ Î±áµ¢ Î±â±¼ yáµ¢ yâ±¼ K(xáµ¢, xâ±¼)
                   SUBJECT TO: Î£ Î±áµ¢ yáµ¢ = 0, 0 â‰¤ Î±áµ¢ â‰¤ C
                   
                   # Use SMO (Sequential Minimal Optimization)
                   REPEAT until convergence:
                       SELECT two variables Î±áµ¢, Î±â±¼ to optimize
                       SOLVE 2-variable sub-problem analytically
                       UPDATE Î±áµ¢, Î±â±¼
                
                4. IDENTIFY support vectors:
                   support_vectors = {xáµ¢ where Î±áµ¢ > 0}
                   
                5. CALCULATE bias:
                   FOR each support vector on margin (0 < Î±áµ¢ < C):
                       b = yáµ¢ - Î£â±¼ Î±â±¼ yâ±¼ K(xâ±¼, xáµ¢)
                   bias = average(all_bias_calculations)
                
                6. RETURN (support_vectors, alpha, bias)
            END
            
            PREDICTION:
            BEGIN
                1. FOR new sample x_new:
                   score = Î£áµ¢ Î±áµ¢ yáµ¢ K(support_vector_i, x_new) + bias
                   
                2. IF classification:
                   prediction = sign(score)
                   
                3. IF regression:
                   prediction = score
                   
                4. RETURN prediction
            END
            ```
            """,
            
            # 7. Python implementation
            'python_implementation': """
            ðŸ”¹ **Python Implementation**
            
            **From Scratch (Simplified Linear SVM):**
            ```python
            import numpy as np
            from scipy.optimize import minimize
            
            class SimpleSVM:
                def __init__(self, C=1.0, max_iter=1000):
                    self.C = C
                    self.max_iter = max_iter
                    self.w = None
                    self.b = None
                    self.support_vectors = None
                    self.alpha = None
                
                def linear_kernel(self, x1, x2):
                    \"\"\"Linear kernel function.\"\"\"
                    return np.dot(x1, x2)
                
                def objective(self, alpha, y, X):
                    \"\"\"Objective function to maximize (dual problem).\"\"\"
                    n = len(alpha)
                    # Compute kernel matrix
                    K = np.zeros((n, n))
                    for i in range(n):
                        for j in range(n):
                            K[i, j] = self.linear_kernel(X[i], X[j])
                    
                    # Dual objective function
                    obj = np.sum(alpha) - 0.5 * np.sum(alpha[:, None] * alpha * y[:, None] * y * K)
                    return -obj  # Minimize negative = maximize
                
                def constraint1(self, alpha, y):
                    \"\"\"Constraint: sum(alpha * y) = 0\"\"\"
                    return np.sum(alpha * y)
                
                def fit(self, X, y):
                    \"\"\"Train the SVM.\"\"\"
                    n_samples, n_features = X.shape
                    
                    # Initial guess for alpha
                    alpha0 = np.random.random(n_samples)
                    
                    # Constraints
                    constraints = [{
                        'type': 'eq',
                        'fun': lambda alpha: self.constraint1(alpha, y)
                    }]
                    
                    # Bounds for alpha (0 <= alpha <= C)
                    bounds = [(0, self.C) for _ in range(n_samples)]
                    
                    # Solve optimization problem
                    result = minimize(
                        fun=lambda alpha: self.objective(alpha, y, X),
                        x0=alpha0,
                        method='SLSQP',
                        bounds=bounds,
                        constraints=constraints
                    )
                    
                    self.alpha = result.x
                    
                    # Find support vectors (alpha > threshold)
                    sv_threshold = 1e-5
                    sv_indices = self.alpha > sv_threshold
                    self.support_vectors = X[sv_indices]
                    self.sv_labels = y[sv_indices]
                    self.sv_alpha = self.alpha[sv_indices]
                    
                    # Calculate weights
                    self.w = np.sum((self.sv_alpha * self.sv_labels)[:, None] * self.support_vectors, axis=0)
                    
                    # Calculate bias using support vectors
                    self.b = np.mean(self.sv_labels - np.dot(self.support_vectors, self.w))
                
                def predict(self, X):
                    \"\"\"Make predictions.\"\"\"
                    scores = np.dot(X, self.w) + self.b
                    return np.sign(scores)
                
                def decision_function(self, X):
                    \"\"\"Return decision scores.\"\"\"
                    return np.dot(X, self.w) + self.b
            
            # Example usage
            from sklearn.datasets import make_classification
            from sklearn.preprocessing import StandardScaler
            
            # Generate data
            X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, 
                                     n_informative=2, random_state=42)
            y[y == 0] = -1  # Convert to -1, +1 labels
            
            # Scale features (important for SVM!)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Train SVM
            svm = SimpleSVM(C=1.0)
            svm.fit(X_scaled, y)
            
            # Make predictions
            predictions = svm.predict(X_scaled)
            ```
            
            **Using Scikit-learn:**
            ```python
            from sklearn.svm import SVC, SVR
            from sklearn.preprocessing import StandardScaler
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score, classification_report
            
            # Classification
            # Data preprocessing (CRITICAL for SVM)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Train SVM classifier
            svm_clf = SVC(
                kernel='rbf',          # RBF kernel for non-linear data
                C=1.0,                # Regularization parameter
                gamma='scale',        # Kernel coefficient
                probability=True,     # Enable probability estimates
                random_state=42
            )
            svm_clf.fit(X_train_scaled, y_train)
            
            # Make predictions
            y_pred = svm_clf.predict(X_test_scaled)
            y_proba = svm_clf.predict_proba(X_test_scaled)
            
            # Get support vectors
            support_vectors = svm_clf.support_vectors_
            n_support = svm_clf.n_support_
            
            print(f"Number of support vectors: {len(support_vectors)}")
            print(f"Support vectors per class: {n_support}")
            
            # Regression
            svm_reg = SVR(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                epsilon=0.1          # Tolerance for regression
            )
            svm_reg.fit(X_train_scaled, y_train)
            y_pred_reg = svm_reg.predict(X_test_scaled)
            ```
            
            **Kernel Examples:**
            ```python
            # Different kernels for different data types
            
            # Linear kernel (for linearly separable data)
            svm_linear = SVC(kernel='linear', C=1.0)
            
            # Polynomial kernel (for polynomial boundaries)
            svm_poly = SVC(kernel='poly', degree=3, C=1.0)
            
            # RBF kernel (most popular, for complex non-linear data)
            svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
            
            # Custom kernel
            def my_kernel(X, Y):
                return np.dot(X, Y.T)  # Same as linear
                
            svm_custom = SVC(kernel=my_kernel)
            ```
            """,
            
            # 8. Example with sample input/output
            'example': """
            ðŸ”¹ **Complete Example: Email Spam Classification**
            
            **Input Data (Email Features):**
            ```
            Email | URGENT_words | Money_words | Length | Links | Spam?
            1     | 0           | 1           | 50     | 1     | No (-1)
            2     | 5           | 3           | 200    | 8     | Yes (+1)
            3     | 1           | 0           | 100    | 2     | No (-1)
            4     | 3           | 5           | 150    | 10    | Yes (+1)
            5     | 0           | 0           | 80     | 0     | No (-1)
            6     | 4           | 2           | 120    | 6     | Yes (+1)
            ```
            
            **Step-by-Step SVM Training:**
            ```
            1. Scale Features (Critical!):
               URGENT_words: [0, 5, 1, 3, 0, 4] â†’ [-1.2, 1.8, -0.8, 0.4, -1.2, 1.0]
               Money_words: [1, 3, 0, 5, 0, 2] â†’ [-0.5, 0.5, -1.0, 2.0, -1.0, 0]
               Length: [50, 200, 100, 150, 80, 120] â†’ [-1.5, 1.8, -0.3, 0.9, -1.0, 0.1]
               Links: [1, 8, 2, 10, 0, 6] â†’ [-1.0, 1.2, -0.8, 1.8, -1.2, 0.6]
            
            2. Find Optimal Hyperplane:
               Goal: Maximize margin between spam (+1) and non-spam (-1)
               
            3. Solve Optimization:
               Found: w = [0.8, 1.2, 0.3, 0.9], b = -0.1
               
            4. Identify Support Vectors:
               Email 2: [5, 3, 200, 8] â†’ Support vector (closest spam)
               Email 3: [1, 0, 100, 2] â†’ Support vector (closest non-spam)
               Email 4: [3, 5, 150, 10] â†’ Support vector (on margin)
               
            5. Decision Boundary:
               0.8Ã—URGENT + 1.2Ã—Money + 0.3Ã—Length + 0.9Ã—Links - 0.1 = 0
            ```
            
            **New Email Prediction:**
            ```
            New Email: URGENT=2, Money=1, Length=75, Links=3
            
            1. Scale features: [2, 1, 75, 3] â†’ [-0.4, -0.5, -1.1, -0.6]
            
            2. Calculate decision score:
               f(x) = 0.8Ã—(-0.4) + 1.2Ã—(-0.5) + 0.3Ã—(-1.1) + 0.9Ã—(-0.6) - 0.1
                    = -0.32 - 0.6 - 0.33 - 0.54 - 0.1 = -1.89
            
            3. Make prediction:
               Since f(x) = -1.89 < 0 â†’ Prediction: NOT SPAM âœ…
               
            4. Confidence:
               Distance from boundary = |f(x)| = 1.89
               High confidence (far from decision boundary)
            ```
            
            **Support Vector Interpretation:**
            ```
            Only 3 out of 6 emails matter for the decision boundary!
            These support vectors define the optimal separation.
            If we remove other emails, decision boundary stays the same.
            ```
            """,
            
            # 9. Visualization explanation
            'visualization': """
            ðŸ”¹ **Understanding Through Visualizations**
            
            **2D Decision Boundary:**
            ðŸ“Š Shows hyperplane separating classes
            â€¢ Solid line = decision boundary
            â€¢ Dashed lines = margin boundaries
            â€¢ Circled points = support vectors
            â€¢ Shaded regions = class predictions
            
            **Margin Visualization:**
            ðŸ“ Shows width of separation corridor
            â€¢ Wider margin = better generalization
            â€¢ Support vectors define margin width
            â€¢ Goal: maximize this margin
            
            **Support Vector Highlighting:**
            ðŸŽ¯ Shows which points matter for decision
            â€¢ Support vectors are critical points
            â€¢ Other points can be removed without changing boundary
            â€¢ Usually only small fraction of data
            
            **Kernel Effect Comparison:**
            ðŸ”„ Side-by-side comparison of different kernels
            â€¢ Linear: Straight line boundaries
            â€¢ Polynomial: Curved boundaries
            â€¢ RBF: Complex, smooth boundaries
            â€¢ Shows how kernel choice affects decision regions
            
            **3D Kernel Transformation:**
            ðŸ“ˆ Shows how kernel maps data to higher dimensions
            â€¢ Original 2D data becomes linearly separable in 3D
            â€¢ Hyperplane in 3D becomes curve in 2D
            â€¢ Illustrates the "kernel trick"
            
            **Regularization Effect (C parameter):**
            âš–ï¸ Shows trade-off between margin and errors
            â€¢ Low C: Wide margin, some misclassifications
            â€¢ High C: Narrow margin, fewer misclassifications
            â€¢ Helps understand bias-variance trade-off
            """,
            
            # 10. Time and space complexity
            'complexity': """
            ðŸ”¹ **Time & Space Complexity**
            
            **Time Complexity:**
            â€¢ **Training**: O(nÂ² Ã— p) to O(nÂ³ Ã— p) where n=samples, p=features
            â€¢ **Prediction**: O(s Ã— p) where s=number of support vectors
            â€¢ **SMO Algorithm**: O(nÂ²) on average, O(nÂ³) worst case
            â€¢ **Kernel Computation**: O(p) per kernel evaluation
            
            **Space Complexity:**
            â€¢ **Model Storage**: O(s Ã— p) where s=support vectors
            â€¢ **Training Memory**: O(nÂ²) for kernel matrix (can be reduced)
            â€¢ **Support Vector Storage**: Typically 10-50% of training data
            
            **Scalability:**
            â€¢ âœ… **High Dimensions**: Excellent with many features
            â€¢ âŒ **Large Datasets**: Poor scaling beyond 10K samples
            â€¢ âœ… **Sparse Data**: Works well with sparse feature vectors
            â€¢ âš ï¸ **Memory Usage**: Kernel matrix can be huge
            â€¢ âœ… **Support Vectors**: Model size independent of training size
            
            **Optimization Notes:**
            â€¢ SMO breaks large problem into series of small 2-variable problems
            â€¢ Working set selection can improve convergence
            â€¢ Kernel caching reduces computation time
            â€¢ Feature scaling essential for performance
            """,
            
            # 11. Advantages and disadvantages
            'pros_cons': """
            ðŸ”¹ **Advantages** âœ…
            â€¢ **High-Dimensional Excellence**: Works great with many features
            â€¢ **Memory Efficient**: Only stores support vectors (not all data)
            â€¢ **Kernel Flexibility**: Can handle complex non-linear patterns
            â€¢ **Robust**: Resistant to overfitting in high dimensions
            â€¢ **Global Optimum**: Convex optimization guarantees global solution
            â€¢ **Theoretical Foundation**: Strong mathematical backing
            â€¢ **Small Dataset Friendly**: Effective with limited training data
            â€¢ **Versatile**: Works for classification and regression
            â€¢ **No Local Minima**: Always finds globally optimal solution
            
            ðŸ”¹ **Disadvantages** âŒ
            â€¢ **Slow on Large Data**: Poor scalability beyond ~10K samples
            â€¢ **Feature Scaling Required**: Very sensitive to feature scales
            â€¢ **No Probability Estimates**: Doesn't naturally output probabilities
            â€¢ **Black Box**: Difficult to interpret decision reasoning
            â€¢ **Hyperparameter Sensitive**: Requires careful tuning of C, gamma
            â€¢ **Memory Intensive**: Kernel matrix can be very large
            â€¢ **No Feature Importance**: Doesn't rank feature importance
            â€¢ **Outlier Sensitive**: Can be affected by noisy data
            â€¢ **Limited to Binary**: Needs modifications for multi-class problems
            """,
            
            # 12. When to use and when NOT to use
            'usage_guide': """
            ðŸ”¹ **When TO Use SVM** âœ…
            
            **Perfect for:**
            â€¢ ðŸ”¬ **High-Dimensional Data**: Text data, genomics (features >> samples)
            â€¢ ðŸ“„ **Text Classification**: Document classification, spam detection
            â€¢ ðŸ–¼ï¸ **Image Recognition**: Face detection, handwriting recognition
            â€¢ ðŸ§¬ **Bioinformatics**: Gene classification, protein analysis
            â€¢ ðŸ“Š **Small Datasets**: When you have limited training data
            â€¢ ðŸŽ¯ **Binary Classification**: Clear two-class problems
            
            **Good when:**
            â€¢ Need robust performance with small datasets
            â€¢ Features are more numerous than samples
            â€¢ Decision boundary is complex but smooth
            â€¢ Data is approximately linearly separable (with kernels)
            â€¢ You want guaranteed global optimum
            
            ðŸ”¹ **When NOT to Use SVM** âŒ
            
            **Avoid when:**
            â€¢ ðŸ“ˆ **Large Datasets**: More than 10,000 samples (use alternatives)
            â€¢ âš¡ **Real-time Predictions**: Need very fast inference
            â€¢ ðŸ” **Interpretability Required**: Need to explain decisions
            â€¢ ðŸ“Š **Probability Estimates**: Need calibrated probability outputs
            â€¢ ðŸŽ¯ **Multi-class Problems**: Many classes (>10)
            â€¢ ðŸ“‰ **Noisy Data**: High noise levels in features or labels
            â€¢ ðŸ’° **Limited Compute**: Constrained computational resources
            
            **Use instead:**
            â€¢ Random Forest (for interpretability)
            â€¢ Logistic Regression (for probabilities)
            â€¢ Gradient Boosting (for structured data)
            â€¢ Neural Networks (for very large datasets)
            â€¢ Naive Bayes (for very fast training/prediction)
            """,
            
            # 13. Common interview questions
            'interview_questions': """
            ðŸ”¹ **Common Interview Questions & Answers**
            
            **Q1: What is the kernel trick and why is it important?**
            A: The kernel trick allows SVM to handle non-linear data without explicitly transforming to higher dimensions:
            â€¢ Instead of Ï†(x)Â·Ï†(y), compute K(x,y) directly
            â€¢ Saves computation and memory
            â€¢ Makes infinite-dimensional mappings possible
            â€¢ Examples: RBF kernel maps to infinite dimensions
            
            **Q2: How do you choose the right kernel?**
            A:
            â€¢ Linear: When data is linearly separable or has many features
            â€¢ Polynomial: When you suspect polynomial relationships
            â€¢ RBF: Default choice, works well for most non-linear data
            â€¢ Sigmoid: Rarely used, similar to neural networks
            â€¢ Try linear first (fastest), then RBF if needed
            
            **Q3: What's the difference between hard margin and soft margin SVM?**
            A:
            â€¢ Hard margin: No misclassification allowed, only works on linearly separable data
            â€¢ Soft margin: Allows some misclassification via slack variables
            â€¢ C parameter controls trade-off: high C â†’ hard margin, low C â†’ soft margin
            â€¢ Soft margin is more practical for real-world noisy data
            
            **Q4: Why is feature scaling crucial for SVM?**
            A: SVM finds optimal hyperplane based on distances:
            â€¢ Features with larger scales dominate the distance calculation
            â€¢ Example: Age (0-100) vs Income (0-100000) - income will dominate
            â€¢ Always use StandardScaler or MinMaxScaler before training
            â€¢ One of the most important preprocessing steps for SVM
            
            **Q5: How does SVM handle multi-class classification?**
            A: SVM is naturally binary, so multi-class needs strategies:
            â€¢ One-vs-One: Train classifier for each pair of classes
            â€¢ One-vs-Rest: Train one classifier per class vs all others
            â€¢ Scikit-learn automatically handles this with decision_function_shape parameter
            """,
            
            # 14. Common mistakes
            'common_mistakes': """
            ðŸ”¹ **Common Beginner Mistakes & How to Avoid**
            
            **Mistake 1: Not scaling features** ðŸš«
            âŒ Training SVM on raw features with different scales
            âœ… **Fix**: Always use StandardScaler() or MinMaxScaler() first
            
            **Mistake 2: Using wrong kernel** ðŸš«
            âŒ Using linear kernel on clearly non-linear data
            âœ… **Fix**: Start with RBF kernel, try linear only for high-dim/sparse data
            
            **Mistake 3: Not tuning hyperparameters** ðŸš«
            âŒ Using default C=1.0 without testing other values
            âœ… **Fix**: Use GridSearchCV to tune C and gamma parameters
            
            **Mistake 4: Using SVM on large datasets** ðŸš«
            âŒ Training SVM on 100K+ samples and wondering why it's slow
            âœ… **Fix**: Use Random Forest or Gradient Boosting for large datasets
            
            **Mistake 5: Expecting probability outputs** ðŸš«
            âŒ Assuming SVM naturally outputs well-calibrated probabilities
            âœ… **Fix**: Use probability=True and consider calibration with CalibratedClassifierCV
            
            **Mistake 6: Ignoring outliers** ðŸš«
            âŒ Not removing obvious outliers that can skew support vectors
            âœ… **Fix**: Clean data and consider robust scaling methods
            
            **Mistake 7: Wrong evaluation metric** ðŸš«
            âŒ Using accuracy on imbalanced datasets
            âœ… **Fix**: Use F1-score, precision, recall for imbalanced classes
            """,
            
            # 15. Comparison with similar algorithms
            'comparisons': """
            ðŸ”¹ **SVM vs Similar Algorithms**
            
            **SVM vs Logistic Regression:**
            â€¢ **SVM**: Maximum margin principle, uses support vectors
            â€¢ **Logistic Regression**: Maximum likelihood, uses all data points
            â€¢ **Use Logistic**: When you need probability estimates
            
            **SVM vs Random Forest:**
            â€¢ **SVM**: Better for high-dimensional data, mathematical guarantees
            â€¢ **Random Forest**: Better for large datasets, interpretable
            â€¢ **Use Random Forest**: For tabular data and feature importance
            
            **SVM vs Neural Networks:**
            â€¢ **SVM**: Convex optimization, no local minima, less data needed
            â€¢ **Neural Networks**: More flexible, better for very complex patterns
            â€¢ **Use Neural Networks**: With large datasets and complex patterns
            
            **SVM vs K-Nearest Neighbors:**
            â€¢ **SVM**: Global model, faster prediction, memory efficient
            â€¢ **KNN**: Local model, no training needed, adapts to local patterns
            â€¢ **Use KNN**: When local patterns matter more than global structure
            
            **Linear SVM vs Kernel SVM:**
            â€¢ **Linear SVM**: Faster training/prediction, works well in high dimensions
            â€¢ **Kernel SVM**: Handles non-linear patterns, more flexible
            â€¢ **Use Linear**: When data is high-dimensional or approximately linear
            """,
            
            # 16. Real-world applications
            'real_world_applications': """
            ðŸ”¹ **Real-World Applications & Industry Use Cases**
            
            **ðŸ“„ Text & Document Analysis:**
            â€¢ Spam email detection and filtering
            â€¢ Document classification and categorization
            â€¢ Sentiment analysis for social media
            â€¢ Language detection in multilingual documents
            â€¢ News article topic classification
            
            **ðŸ–¼ï¸ Computer Vision & Image Processing:**
            â€¢ Face detection and recognition systems
            â€¢ Handwritten digit recognition (postal services)
            â€¢ Medical image analysis (X-ray, MRI classification)
            â€¢ Object detection in security cameras
            â€¢ Quality control in manufacturing
            
            **ðŸ§¬ Bioinformatics & Healthcare:**
            â€¢ Gene classification and protein analysis
            â€¢ Drug discovery and molecular design
            â€¢ Disease diagnosis from symptoms
            â€¢ Cancer detection from tissue samples
            â€¢ Personalized medicine recommendations
            
            **ðŸ’° Finance & Risk Management:**
            â€¢ Credit scoring and loan approval
            â€¢ Fraud detection in transactions
            â€¢ Stock market prediction
            â€¢ Risk assessment for insurance
            â€¢ Algorithmic trading strategies
            
            **ðŸ”¬ Scientific Research:**
            â€¢ Particle physics data analysis
            â€¢ Climate change modeling
            â€¢ Chemical compound classification
            â€¢ Astronomical object detection
            â€¢ Materials science property prediction
            
            **ðŸ“± Technology & Software:**
            â€¢ Search engine ranking algorithms
            â€¢ Recommendation system components
            â€¢ Network intrusion detection
            â€¢ Software bug prediction
            â€¢ User behavior classification
            
            **ðŸ­ Manufacturing & Quality Control:**
            â€¢ Defect detection in production lines
            â€¢ Predictive maintenance systems
            â€¢ Process optimization in chemical plants
            â€¢ Supply chain risk assessment
            â€¢ Equipment failure prediction
            
            **ðŸ’¡ Key Success Factors:**
            â€¢ Proper feature scaling and preprocessing
            â€¢ Careful hyperparameter tuning
            â€¢ Appropriate kernel selection for data type
            â€¢ Sufficient but not excessive training data
            â€¢ Domain expertise for feature engineering
            """
        }
    
    def generate_sample_data(self, task_type, n_samples=300, n_features=4):
        """Generate sample data for demonstration."""
        if task_type == 'classification':
            X, y = make_classification(
                n_samples=n_samples,
                n_features=n_features,
                n_redundant=0,
                n_informative=n_features,
                random_state=42
            )
        else:
            X, y = make_regression(
                n_samples=n_samples,
                n_features=n_features,
                noise=0.1,
                random_state=42
            )
        return X, y
    
    def fit(self, X, y):
        """Fit the SVM model."""
        if self.task_type == 'classification':
            self.model = SVC(kernel=self.kernel, C=self.C, random_state=42)
        else:
            self.model = SVR(kernel=self.kernel, C=self.C)
        
        self.model.fit(X, y)
        self.is_fitted = True
        return self
    
    def predict(self, X):
        """Make predictions using the fitted model."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")
        return self.model.predict(X)
    
    def get_metrics(self, X, y):
        """Calculate performance metrics for the model."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before calculating metrics")
        
        y_pred = self.predict(X)
        
        if self.task_type == 'classification':
            metrics = {
                'Accuracy': accuracy_score(y, y_pred),
                'Precision': precision_score(y, y_pred, average='weighted', zero_division=0),
                'Recall': recall_score(y, y_pred, average='weighted', zero_division=0),
                'F1-Score': f1_score(y, y_pred, average='weighted', zero_division=0)
            }
        else:  # regression
            metrics = {
                'MSE': mean_squared_error(y, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y, y_pred)),
                'MAE': mean_absolute_error(y, y_pred),
                'RÂ² Score': r2_score(y, y_pred)
            }
        
        return metrics
    
    def streamlit_interface(self):
        """Create comprehensive Streamlit interface for Support Vector Machine."""
        st.subheader("ðŸŽ¯ Support Vector Machine (SVM)")
        
        theory = self.get_theory()
        
        # Main tabs for comprehensive coverage
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "ðŸŽ¯ Overview", "ðŸ“š Deep Dive", "ðŸ’» Implementation", 
            "ðŸ§ª Interactive Demo", "â“ Q&A", "ðŸ¢ Applications"
        ])
        
        with tab1:
            # Overview Tab - Essential Information
            st.markdown("### ðŸŽ¯ What is Support Vector Machine?")
            st.markdown(theory['definition'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### ðŸŒŸ Why Use It?")
                st.markdown(theory['motivation'])
                
            with col2:
                st.markdown("### ðŸŽ‰ Simple Analogy")
                st.markdown(theory['intuition'])
            
            # Quick advantages/disadvantages
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### âœ… Pros")
                st.markdown(theory['pros_cons'].split('ðŸ”¹ **Disadvantages**')[0])
                
            with col2:
                st.markdown("### âŒ Cons")
                if 'ðŸ”¹ **Disadvantages**' in theory['pros_cons']:
                    st.markdown("ðŸ”¹ **Disadvantages**" + theory['pros_cons'].split('ðŸ”¹ **Disadvantages**')[1])
        
        with tab2:
            # Deep Dive Tab - Mathematical and Technical Details
            st.markdown("### ðŸ“Š Mathematical Foundation")
            st.markdown(theory['math_foundation'])
            
            st.markdown("### ðŸ”„ Algorithm Steps")
            st.markdown(theory['algorithm_steps'])
            
            st.markdown("### ðŸ’¾ Pseudocode")
            st.markdown(theory['pseudocode'])
            
            st.markdown("### âš¡ Time & Space Complexity")
            st.markdown(theory['complexity'])
            
        with tab3:
            # Implementation Tab
            st.markdown("### ðŸ’» Python Implementation")
            st.markdown(theory['python_implementation'])
            
            st.markdown("### ðŸ“‹ Complete Example")
            st.markdown(theory['example'])
            
            st.markdown("### ðŸ“ˆ Visualization Guide")
            st.markdown(theory['visualization'])
        
        with tab4:
            # Interactive Demo Tab
            st.markdown("### ðŸ§ª Try Support Vector Machine Yourself!")
            self._create_interactive_demo()
        
        with tab5:
            # Q&A Tab
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### ðŸŽ¯ When to Use")
                st.markdown(theory['usage_guide'])
                
                st.markdown("### ðŸš« Common Mistakes")
                st.markdown(theory['common_mistakes'])
                
            with col2:
                st.markdown("### â“ Interview Questions")
                st.markdown(theory['interview_questions'])
                
                st.markdown("### âš–ï¸ Algorithm Comparisons")
                st.markdown(theory['comparisons'])
        
        with tab6:
            # Applications Tab
            st.markdown("### ðŸŒ Real-World Applications")
            st.markdown(theory['real_world_applications'])
    
    def _create_interactive_demo(self):
        """Create the interactive demo section."""
        
        # Parameters section
        st.markdown("### ðŸ”§ Parameters")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            task_type = st.selectbox("Task Type:", ['classification', 'regression'])
        with col2:
            kernel = st.selectbox("Kernel:", ['linear', 'poly', 'rbf', 'sigmoid'])
        with col3:
            C = st.slider("Regularization (C):", 0.1, 10.0, 1.0, 0.1)
        with col4:
            n_samples = st.slider("Samples:", 100, 500, 300)
        
        # Update parameters
        self.task_type = task_type
        self.kernel = kernel
        self.C = C
        
        # Generate data and train model
        X, y = self.generate_sample_data(task_type, n_samples)
        
        # Feature scaling (important for SVM)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42)
        
        self.fit(X_train, y_train)
        
        # Results section
        st.markdown("### ðŸ“Š Results")
        
        # Basic metrics
        train_metrics = self.get_metrics(X_train, y_train)
        test_metrics = self.get_metrics(X_test, y_test)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Training Metrics:**")
            for metric_name, value in train_metrics.items():
                st.metric(metric_name, f"{value:.4f}")
                
        with col2:
            st.markdown("**Test Metrics:**")
            for metric_name, value in test_metrics.items():
                st.metric(metric_name, f"{value:.4f}")
        
        # Support vectors information
        if hasattr(self.model, 'support_vectors_'):
            st.markdown("### ðŸŽ¯ Support Vector Information")
            col1, col2, col3 = st.columns(3)
            col1.metric("Number of Support Vectors", len(self.model.support_vectors_))
            if hasattr(self.model, 'n_support_'):
                col2.metric("Support Vectors per Class", str(self.model.n_support_))
            if hasattr(self.model, 'dual_coef_'):
                col3.metric("Dual Coefficients Shape", str(self.model.dual_coef_.shape))
        
        # Interpretation
        st.markdown("### ðŸ” Interpretation")
        
        if task_type == 'classification':
            if 'Accuracy' in test_metrics:
                accuracy = test_metrics['Accuracy']
                if accuracy > 0.9:
                    st.success(f"**Excellent performance!** Accuracy: {accuracy:.3f}")
                    st.write("The SVM is performing very well on this dataset.")
                elif accuracy > 0.8:
                    st.info(f"**Good performance.** Accuracy: {accuracy:.3f}")
                    st.write("The SVM is performing well.")
                elif accuracy > 0.7:
                    st.warning(f"**Moderate performance.** Accuracy: {accuracy:.3f}")
                    st.write("Consider tuning hyperparameters or trying different kernels.")
                else:
                    st.error(f"**Poor performance.** Accuracy: {accuracy:.3f}")
                    st.write("The dataset might not be suitable for SVM or needs different preprocessing.")
        else:  # regression
            if 'RÂ² Score' in test_metrics:
                r2_score_val = test_metrics['RÂ² Score']
                if r2_score_val > 0.8:
                    st.success(f"**Excellent fit!** RÂ² Score: {r2_score_val:.3f}")
                elif r2_score_val > 0.6:
                    st.info(f"**Good fit.** RÂ² Score: {r2_score_val:.3f}")
                elif r2_score_val > 0.3:
                    st.warning(f"**Moderate fit.** RÂ² Score: {r2_score_val:.3f}")
                else:
                    st.error(f"**Poor fit.** RÂ² Score: {r2_score_val:.3f}")
        
        # Model characteristics
        st.markdown("**Model Characteristics:**")
        col1, col2, col3 = st.columns(3)
        col1.metric("Kernel", kernel.upper())
        col2.metric("Regularization (C)", f"{C}")
        col3.metric("Task", task_type.capitalize())


def main():
    """Main function for testing Support Vector Machine."""
    svm = SupportVectorMachine()
    svm.streamlit_interface()


if __name__ == "__main__":
    main()